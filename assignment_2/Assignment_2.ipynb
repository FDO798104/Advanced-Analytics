{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkst-SF8aDkE"
      },
      "outputs": [],
      "source": [
        "!pip install keras\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip install json\n",
        "!pip install tensorflow_io\n",
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCBENZmIkBMa"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm import tqdm\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from PIL import Image\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available? - \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC3XFr7SkD61"
      },
      "outputs": [],
      "source": [
        "# Extract images from ZIP and store in drive\n",
        "# Use custom images_small_jpg.zip - extraction time +/- 5 minutes\n",
        "dataset_path = '/content/drive/MyDrive/dataset.json'\n",
        "# zip_path = '/content/drive/MyDrive/images-small.zip'\n",
        "zip_path = '/content/drive/MyDrive/images_small_jpg.zip'\n",
        "images_path = '/content/jpg'\n",
        "\n",
        "# Check if the extraction directory already exists\n",
        "if not os.path.exists(images_path):\n",
        "    os.mkdir(images_path)\n",
        "# Extract zip file store it on disk (this will be deleted after runtime ends. Extracting is always needed.)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(images_path)\n",
        "print(\"Zip file extracted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrfdIr3JkLvf"
      },
      "outputs": [],
      "source": [
        "# Set to True for 22 categories (0-99, 100-199, ...)\n",
        "# Set to False for 4 categories (very cheap-cheap-expensive-very expensive)\n",
        "use_22_cats = False\n",
        "\n",
        "\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "prices_id = {}\n",
        "screenshot_ids = []\n",
        "all_prices_list = []\n",
        "for item in data:\n",
        "    # Check if 'price_category' key is present in the 'item' dictionary\n",
        "    if item['price'] is not None:\n",
        "        # Extract the price category\n",
        "        price = item['price']\n",
        "        all_prices_list.append(price)\n",
        "\n",
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = np.percentile(all_prices_list, 25)\n",
        "Q2 = np.percentile(all_prices_list, 50)\n",
        "Q3 = np.percentile(all_prices_list, 75)\n",
        "Q4 = np.percentile(all_prices_list, 100)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the outlier boundaries\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "price_cats = dict()\n",
        "x = 0\n",
        "# Use for 22 categories...\n",
        "while x <= upper_bound:\n",
        "  price_cats[x]=f'{x}-{x+99}'\n",
        "  x += 100\n",
        "print(price_cats)\n",
        "\n",
        "if not use_22_cats:\n",
        "  price_cats = {int(Q1): 'v-cheap', int(Q2): 'cheap', int(Q3): 'expensive', int(Q4): 'v-expensive'}\n",
        "\n",
        "for item in data:\n",
        "    # Check if 'price_category' key is present in the 'item' dictionary\n",
        "    if item['price'] is not None:\n",
        "        # Extract the price category\n",
        "        price = item['price']\n",
        "        if price > upper_bound:\n",
        "          print(\"outlier ignored\")\n",
        "          continue\n",
        "        rounded_price = math.floor(price / 100) * 100\n",
        "\n",
        "        # Extract the image IDs from the \"full_images\" list and map them to their price category\n",
        "        if lower_bound <= price <= upper_bound:\n",
        "          for screenshot in item[\"screenshots\"]:\n",
        "              screenshot_id = screenshot\n",
        "              screenshot_ids.append(screenshot_id)\n",
        "              if use_22_cats:\n",
        "                prices_id[screenshot_id] = price_cats[rounded_price]\n",
        "              else:\n",
        "                selected_key = None\n",
        "                for key in reversed(price_cats.keys()):\n",
        "                  if price <= int(key):\n",
        "                    selected_key = key\n",
        "                  else:\n",
        "                    break\n",
        "                if selected_key == None:\n",
        "                  selected_key = Q4\n",
        "                prices_id[screenshot_id] = price_cats[selected_key]\n",
        "\n",
        "price_counts = pd.Series(prices_id.values()).value_counts()\n",
        "downsample_value = 1 * price_counts.mean()\n",
        "if not use_22_cats:\n",
        "  # downsample to min to get all categries of equal size\n",
        "  downsample_value = price_counts.min()\n",
        "print(downsample_value)\n",
        "# Create a DataFrame from the value counts\n",
        "df_counts = pd.DataFrame({'Price_Category': price_counts.index, 'Count': price_counts.values})\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(df_counts['Price_Category'], df_counts['Count'])\n",
        "plt.title('Bar Chart of Price Category Counts')\n",
        "plt.xlabel('Price Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Code for boxplots\n",
        "# prices = sorted(filtered_data.values())\n",
        "# # Define the bin edges\n",
        "# max_price = max(prices)\n",
        "# bins = np.arange(0, max_price + 100, 100)\n",
        "\n",
        "# # Count the prices in each bin\n",
        "# hist, bin_edges = np.histogram(prices, bins=bins)\n",
        "\n",
        "# # Plot the data\n",
        "# plt.bar(bin_edges[:-1], hist, width=10, edgecolor='black')\n",
        "\n",
        "# # Label the plot\n",
        "# plt.xlabel('Price Range')\n",
        "# plt.ylabel('Number of Prices')\n",
        "# plt.title('Distribution of Prices in Blocks of 10')\n",
        "# plt.xticks(bin_edges)\n",
        "\n",
        "# # Display the plot\n",
        "# plt.show()\n",
        "\n",
        "examples = 0\n",
        "for id, price in prices_id.items():\n",
        "  print(f'price: {price} - id: {id}')\n",
        "  examples += 1\n",
        "  if examples >= 10:\n",
        "    print('.....')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sLb9QWSbSJv"
      },
      "outputs": [],
      "source": [
        "images_path = '/content/jpg/images_jpg'\n",
        "train_set_path = '/content/train/'\n",
        "validation_set_path = '/content/validation/'\n",
        "test_set_path = '/content/test/'\n",
        "\n",
        "try:\n",
        "  os.mkdir(train_set_path)\n",
        "  os.mkdir(validation_set_path)\n",
        "  os.mkdir(test_set_path)\n",
        "except:\n",
        "  print(\"files exist\")\n",
        "\n",
        "use_small = False # Set to True for testing and troubleshooting to spare resources\n",
        "\n",
        "if use_small:\n",
        "  train_split = 0.07\n",
        "  val_split = 0.015\n",
        "  test_split = 0.015\n",
        "else:\n",
        "  train_split = 0.7\n",
        "  val_split = 0.15\n",
        "  test_split = 0.15\n",
        "\n",
        "\n",
        "random.seed(42) # random seed (42 - the answer to the ultimate question of life the universe and everything ;) )\n",
        "random.shuffle(screenshot_ids)  # shuffle image id's\n",
        "\n",
        "total_data_df = pd.DataFrame({'id': screenshot_ids, \"label\": [prices_id[scr_shot_id] for scr_shot_id in screenshot_ids]})\n",
        "print(total_data_df.head())\n",
        "\n",
        "# Define the number of samples you want to downsample to for each class\n",
        "desired_samples = int(round(downsample_value))  # Adjust as needed\n",
        "print(desired_samples)\n",
        "# Perform downsampling for each class\n",
        "downsampled_data = []\n",
        "for label_class in total_data_df['label'].unique():\n",
        "    # Select data points belonging to the current class\n",
        "    class_data = total_data_df[total_data_df['label'] == label_class]\n",
        "\n",
        "    # Downsample the class to the desired number of samples\n",
        "    if len(class_data) >= desired_samples:\n",
        "      downsampled_class = resample(class_data, replace=False, n_samples=desired_samples, random_state=42)\n",
        "    else:\n",
        "      downsampled_class = class_data\n",
        "    # Add the downsampled class to the list\n",
        "    downsampled_data.append(downsampled_class)\n",
        "\n",
        "# Concatenate the downsampled classes into a single DataFrame\n",
        "downsampled_df = pd.concat(downsampled_data)\n",
        "\n",
        "# Shuffle the downsampled DataFrame\n",
        "total_data_df = downsampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "label_counts = total_data_df['label'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(label_counts.index, label_counts.values)\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bar Chart of Label Counts')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
        "plt.show()\n",
        "\n",
        "train_df, temp_data = train_test_split(total_data_df, test_size=(1 - train_split), random_state=42)\n",
        "\n",
        "# Now, split the temporary set into validation and test sets\n",
        "val_size = val_split / (val_split + test_split)  # Adjust the validation size to the proportion of the temp set\n",
        "val_df, test_df = train_test_split(temp_data, test_size=(1 - val_size), random_state=42)\n",
        "\n",
        "train_filenames = train_df['id'].tolist()\n",
        "val_filenames = val_df['id'].tolist()\n",
        "test_filenames = test_df['id'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMlrT3Iv22U6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def is_image_corrupted(image_path):\n",
        "    try:\n",
        "        Image.open(image_path)\n",
        "        return False\n",
        "    except (IOError, SyntaxError) as e:\n",
        "      # print(e)\n",
        "      return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2JYQNeC7Tug"
      },
      "outputs": [],
      "source": [
        "# execution time for large: approx 4 min (je pense)\n",
        "# execution time for small: 1 (idk - tis ni lang)\n",
        "corrupted_count = 0\n",
        "for filename in train_filenames:\n",
        "    src_path = os.path.join(images_path, filename)\n",
        "    dst_path = os.path.join(train_set_path, filename)\n",
        "    is_corrupted = is_image_corrupted(src_path)\n",
        "    if is_corrupted:\n",
        "        #print(f\"corrupted image: {filename}\")\n",
        "        corrupted_count += 1\n",
        "    else:\n",
        "        shutil.move(src_path, dst_path)\n",
        "print(f'corrupted or missing files in train set: {corrupted_count}')\n",
        "\n",
        "corrupted_count = 0\n",
        "for filename in val_filenames:\n",
        "    src_path = os.path.join(images_path, filename)\n",
        "    dst_path = os.path.join(validation_set_path, filename)\n",
        "    is_corrupted = is_image_corrupted(src_path)\n",
        "    if is_corrupted:\n",
        "        # print(f\"corrupted image: {filename}\")\n",
        "        corrupted_count += 1\n",
        "    else:\n",
        "        shutil.move(src_path, dst_path)\n",
        "print(f'corrupted or missing files in val set: {corrupted_count}')\n",
        "\n",
        "corrupted_count = 0\n",
        "for filename in test_filenames:\n",
        "    src_path = os.path.join(images_path, filename)\n",
        "    dst_path = os.path.join(test_set_path, filename)\n",
        "    is_corrupted = is_image_corrupted(src_path)\n",
        "    if is_corrupted:\n",
        "        # print(f\"corrupted image: {filename}\")\n",
        "        corrupted_count += 1\n",
        "    else:\n",
        "        shutil.move(src_path, dst_path)\n",
        "print(f'corrupted or missing files in test set: {corrupted_count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZEdybfr7Wcy"
      },
      "outputs": [],
      "source": [
        "img_width, img_height = 224, 224\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfUImtcBAFtK"
      },
      "outputs": [],
      "source": [
        "# Build generators, without all data should be loaded in RAM. This is not possible so we use a generator\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255, rotation_range=20, fill_mode='nearest')\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVnh1RJMAhXE"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train_df,\n",
        "        directory='/content/train',\n",
        "        target_size=(224,224),\n",
        "        x_col=\"id\",\n",
        "        y_col=\"label\",\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        class_mode=\"categorical\",\n",
        "        batch_size=32)\n",
        "\n",
        "validation_generator = val_datagen.flow_from_dataframe(\n",
        "        dataframe = val_df,\n",
        "        directory='/content/validation',\n",
        "        target_size=(224,224),\n",
        "        x_col=\"id\",\n",
        "        y_col=\"label\",\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        class_mode=\"categorical\",\n",
        "        batch_size=32)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = test_df,\n",
        "        directory='/content/test',\n",
        "        target_size=(224,224),\n",
        "        x_col=\"id\",\n",
        "        y_col=\"label\",\n",
        "        seed=42,\n",
        "        shuffle=False,\n",
        "        class_mode=\"categorical\",\n",
        "        batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model:"
      ],
      "metadata": {
        "id": "oU9rWOgvUtV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOxuheYzD4oz"
      },
      "outputs": [],
      "source": [
        "# Experiment with setup. This one is from an internet example. Activation function i would keep at relu or leaky relu or gelu\n",
        "# Final function should be linear for regression\n",
        "# Before running ANY experiment with GPU. Check in resources tab to see if you have enough estimated processing time!\n",
        "# It might be an idea to implement a save model after every epoch but this will take up a lot of space in drive so only if absolutly necessary\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "num_classes = len(price_cats.keys())\n",
        "model.add(Dense(num_classes, activation='softmax')) #softmax for multiclass, linear for regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epfB5NOWEBJL"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfST90fNEFuU"
      },
      "outputs": [],
      "source": [
        "# Train for one epoch for test\n",
        "history = model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=1, validation_data=validation_generator, validation_steps=len(validation_generator))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)"
      ],
      "metadata": {
        "id": "T_HEu4p3nBYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get true labels from the generator\n",
        "true_labels = test_generator.classes\n"
      ],
      "metadata": {
        "id": "8bsJiBoRHrWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(generator, model, num_images=10):\n",
        "    # Get class indices from the generator\n",
        "    class_indices = generator.class_indices\n",
        "    # Create a reverse mapping from indices to class names\n",
        "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
        "    print(idx_to_class)\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = next(generator)\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(60, 4))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        # print(labels[i])\n",
        "        index_of_labels_array = np.where(labels[i] == 1)[0][0]\n",
        "        true_label_name = idx_to_class[index_of_labels_array]\n",
        "        pred_label_name = idx_to_class[predicted_labels[i]]\n",
        "        plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_predictions(test_generator, model, num_images=30)"
      ],
      "metadata": {
        "id": "4JgL3t57727P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSTg57Efi3n"
      },
      "outputs": [],
      "source": [
        "# keep\n",
        "# this\n",
        "# code\n",
        "# but\n",
        "# don't\n",
        "# use\n",
        "# it\n",
        "# ;)\n",
        "\n",
        "# Keep this code block in the file but don't use it unless necessary\n",
        "raise FileExistsError(\"The file \\'images_small_jpg.zip\\' was already created and this code block should not be ran again. Make sure you have a shortcut to the zipfile in \\'MyDrive\\'\")\n",
        "# Keras library does not support webp (so try and convert to jpg hoping file sizes remain small enoug, or alternative is custom vectorization (which will be a pain in the ass))\n",
        "# This will take approx 4 hours so let's try working with WEBP\n",
        "images_path = '/content/images/'\n",
        "jpg_images_path = '/content/images/images_jpg'\n",
        "try:\n",
        "  os.mkdir(jpg_images_path)\n",
        "except FileExistsError:\n",
        "  print(\"Folder already exists\")\n",
        "files = os.listdir(images_path)\n",
        "total_iterations = len(files)\n",
        "progress_bar = tqdm(total=total_iterations, desc=\"Processing\")\n",
        "images = os.listdir(images_path)\n",
        "for image in images:\n",
        "  progress_bar.update(1)\n",
        "  image_jpg = image.replace('.webp', '.jpg')\n",
        "  # Convert to JPEG\n",
        "  jpeg_image_path = os.path.join(jpg_images_path, image_jpg)\n",
        "  if os.path.exists(jpeg_image_path):\n",
        "    continue\n",
        "  webp_image_path = os.path.join(images_path, image)\n",
        "  try:\n",
        "    webp_image = Image.open(webp_image_path)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "  webp_image.convert('RGB').save(jpeg_image_path, 'JPEG')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep this code block in the file but don't use it unless necessary\n",
        "raise FileExistsError(\"The file \\'images_small_jpg.zip\\' was already created and this code block should not be ran again. Make sure you have a shortcut to the zipfile in \\'MyDrive\\'\")\n",
        "import zipfile\n",
        "import os\n",
        "jpg_images_path = '/content/images/images_jpg'\n",
        "output_path = 'content/drive/MyDrive/images_small_jpg.zip'\n",
        "\n",
        "def zip_folder(folder_path, output_path):\n",
        "    # Create a zip file with write permission\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Walk through all the directories and files in the folder\n",
        "        #for root, dirs, files in os.walk(folder_path):\n",
        "        files = os.listdir(jpg_images_path)\n",
        "        for file in files:\n",
        "            # Create the complete filepath by joining root with the file name\n",
        "            file_path = os.path.join(jpg_images_path, file)\n",
        "            # Add file to the zip file\n",
        "            # The arcname parameter avoids storing the full path in the zip file\n",
        "            zipf.write(file_path, arcname=os.path.relpath(file_path, os.path.dirname(folder_path)))\n",
        "\n",
        "# Specify the path to the directory to be zipped\n",
        "jpg_images_path = '/content/images/images_jpg'\n",
        "# Specify the output path for the zip file\n",
        "zip_output_path = '/content/drive/MyDrive/images_small_jpg.zip'\n",
        "\n",
        "# Call the function\n",
        "zip_folder(jpg_images_path, zip_output_path)\n",
        "\n",
        "print(\"Folder successfully zipped!\")"
      ],
      "metadata": {
        "id": "vgFTEN4XzsUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}