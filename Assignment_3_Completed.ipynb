{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3: Streaming analytics on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "newest?next=40481209&n=31\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "newest?next=40480799&n=61\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "newest?next=40480359&n=91\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Appended1: Ask HN: No BS, which is actually faster C++ or Rust? & 2024-05-26 08:18:51\n",
      "Appended1: Show HN: I made a state machine framework for guided conversations with LLMs & 2024-05-26 08:18:51\n",
      "Appended1: LLMs Aren't Sentient & 2024-05-26 08:18:51\n",
      "Appended1: Ask HN: How to add INR/custom currency formats in MS Excel & 2024-05-26 08:18:51\n",
      "Appended1: I Read the Entire Neovim User Manual & 2024-05-26 08:18:51\n",
      "newest?next=40480067&n=121\n",
      "Appended1: ImportGenius & 2024-05-26 08:18:51\n",
      "Appended1: Nobody wants to work with the best engineer: Kindness is underrated & 2024-05-26 08:18:51\n",
      "Appended1: A DOS Format Blunder Revealed Some Priceless Source Code & 2024-05-26 08:18:51\n",
      "Appended1: Hurl, the Exceptional Language & 2024-05-26 08:18:51\n",
      "False1.2\n",
      "newest?next=40479726&n=151\n",
      "---------------\n",
      "Skipped\n",
      "Appended2: UpCodes & 2024-05-26 14:02:51\n",
      "Appended2: 'I was misidentified as shoplifter by facial recognition tech' & 2024-05-26 09:18:51\n",
      "Skipped\n",
      "Appended2: Turn Your iPhone into a Dumb Phone & 2024-05-26 10:18:51\n",
      "Skipped\n",
      "Appended2: How Home Assistant is being used to protect from missile and drone attacks & 2024-05-26 06:18:51\n",
      "Appended2: Call to Action: Fediverse Media Server & 2024-05-26 10:18:51\n",
      "Skipped\n",
      "Appended2: GRC SpinRite & 2024-05-26 03:18:51\n",
      "Skipped\n",
      "Appended2: The Algorithm Behind Jim Simons's Success & 2024-05-26 04:18:51\n",
      "Appended2: Electrically conductive bricks can replace fossil fuels in industrial processes & 2024-05-26 12:18:51\n",
      "Skipped\n",
      "Appended2: How do layer 2s differ from execution sharding? & 2024-05-26 03:18:51\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Appended2: The accidental tyranny of user interfaces & 2024-05-26 11:18:51\n",
      "Skipped\n",
      "Skipped\n",
      "?p=2\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Appended2: Fixing My Yamaha Electone Me-50: An FM Synthesizer Home Organ from 1986 & 2024-05-26 03:18:51\n",
      "Skipped\n",
      "Appended2: I was at Woodstock '99 and it destroyed my innocence (2022) & 2024-05-26 06:18:51\n",
      "Appended2: Mersenne Twister PRNG for JavaScript & 2024-05-26 06:18:51\n",
      "Skipped\n",
      "Skipped\n",
      "Appended2: Hacking phones is too easy & 2024-05-26 08:18:51\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Appended2: (FinTech) Synapse has collapsed, 10M consumers, 100s of fintechs in trouble & 2024-05-26 09:18:51\n",
      "Appended2: Pong Wars with an End & 2024-05-26 05:18:51\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=3\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=4\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=5\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=6\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=7\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=8\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "Skipped\n",
      "?p=9\n",
      "Ended\n"
     ]
    }
   ],
   "source": [
    "#So we can work easier with posted_at/age of the post\n",
    "def age_to_time(i):\n",
    "    if 'minute' in i:\n",
    "        return int(i.split()[0])\n",
    "    elif 'hour' in i:\n",
    "        return int(i.split()[0]) * 60\n",
    "    elif 'day' in i:\n",
    "        return int(i.split()[0]) * 1440\n",
    "    else:\n",
    "        return 10000 #With this we will be sure that we will throw away the var\n",
    "\n",
    "#Here we get the title or header of the site\n",
    "def contains_desired_class(class_attr):\n",
    "    return class_attr and re.search(r'\\b(header|title|name|headline)\\b', class_attr)\n",
    "\n",
    "#we loop over both the Newest and New (Front) page\n",
    "#We will loop over 1 to illustrate, when scraping this would be 1000.\n",
    "for v in range(1):\n",
    "    time.sleep(300)\n",
    "    #Define the date now\n",
    "    current_date = dt.datetime.now()\n",
    "    #Go through the already made files\n",
    "    pattern = re.compile(r\"New_file(\\d+)\")\n",
    "    files = os.listdir('.')\n",
    "\n",
    "    # Initialize the variable to keep track of the highest number\n",
    "    max_number = -1\n",
    "\n",
    "    # Iterate over the files and find the highest number\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            if number > max_number:\n",
    "                max_number = number\n",
    "    print(max_number)\n",
    "    g = max_number\n",
    "    \n",
    "    #Reset all variables    \n",
    "    Website = requests.get(\"https://news.ycombinator.com/newest\")\n",
    "    soup = BeautifulSoup(Website.text, 'html.parser') \n",
    "    #Make an all_data list\n",
    "    all_data = []\n",
    "    #Take the saved data in a list to all_data\n",
    "    with open(f\"New_file{g}.json\", \"r\") as save_file:  \n",
    "        all_data = json.load(save_file)\n",
    "\n",
    "    #start off with both conditions being True\n",
    "    cond1 = True\n",
    "    cond2 = True\n",
    "\n",
    "    #NEWEST\n",
    "    #This condition will change when we reach 12h the first loop, \n",
    "    #or until we reach a duplicate in the second or until we dont find any more tables (website error).\n",
    "    while cond1 and cond2:\n",
    "        time.sleep(10)\n",
    "        table = soup.find('table')\n",
    "\n",
    "        if table:\n",
    "            for index, (title_soup,posted_at_new,user_new,votes_new,comment_soup) in enumerate(zip(table.find_all('span', attrs={'class':'titleline'}),\n",
    "                table.find_all('span', attrs={'class':'age'}), table.find_all('a', attrs={'class':'hnuser'}),\n",
    "                table.find_all('span', attrs={'class':'score'}), table.find_all('td', attrs={'class':'subtext'}))):\n",
    "                #Put all values back to Null\n",
    "                title = None\n",
    "                url = None\n",
    "                domain = None\n",
    "                posted_at = None\n",
    "                user = None\n",
    "                votes = None \n",
    "                comment = None\n",
    "                source_title = None\n",
    "                source_text = None\n",
    "                cont_cond = False\n",
    "                break_cond = False\n",
    "\n",
    "                #Define posted_at first, so we can skip the whole loop if not relevant\n",
    "                posted_at = age_to_time(posted_at_new.get_text())\n",
    "                if 359 < posted_at < 721:\n",
    "                    pass\n",
    "                elif posted_at < 360 :\n",
    "                    print(\"Skipped\") #Interesting to see whether the entry was skipped or not\n",
    "                    continue\n",
    "                else: \n",
    "                    cond2 = False\n",
    "                    print(\"False2\") #This may only happen the first time through the loop\n",
    "                    break\n",
    "                \n",
    "                #The date the post was gathered\n",
    "                post_date = current_date - timedelta(minutes = posted_at)\n",
    "                posted_at_date = post_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                #Find the Title\n",
    "                match = re.match(r\"^(.*?) (?:\\[(?:video|.+?)\\] )?\\(([a-zA-Z].*?)\\)\", title_soup.text)\n",
    "                if match:\n",
    "                    title, domain_new = match.groups()\n",
    "                else:\n",
    "                    title = title_soup.text\n",
    "                    domain_new = None\n",
    "\n",
    "                #Define the User and Vote\n",
    "                user = user_new.get_text()\n",
    "                votes = votes_new.get_text()\n",
    "\n",
    "                #go through the file, and if it is a dupe stop the whole while loop.\n",
    "                for item in all_data:\n",
    "                    if item['Title'] == title and item['User'] == user:\n",
    "                        #If one or multiple posts just appear while going through them they might get \n",
    "                        #displaced from one page to another, if something like this is not in function \n",
    "                        #it will just stop scanning. This is to prevent that.\n",
    "                        if index in (0, 1, 2): \n",
    "                            print(\"False1.1\")\n",
    "                            cont_cond = True\n",
    "                            break #just breaks the for loop we are in now\n",
    "                        else:\n",
    "                            print(\"False1.2\")\n",
    "                            cond1 = False\n",
    "                            break_cond = True\n",
    "                            break #just breaks the for loop we are in now\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                #If it is an index 0, 1 or 2 (the few first posts), there is still a possibility\n",
    "                #too much time passed scraping and the posts went to the next page and the other\n",
    "                #posts on the page are relevant, if it is not the first few, we reached a duplicate,\n",
    "                #so we have to break the whole loop and the scraping of Newest will stop.\n",
    "                if cont_cond == True:\n",
    "                    continue\n",
    "                if break_cond == True:\n",
    "                    break\n",
    "\n",
    "                #Find the Url\n",
    "                title_link = title_soup.find('a')\n",
    "                if title_link:\n",
    "                    url = title_link['href']\n",
    "                try: \n",
    "                    url_req = requests.get(url)\n",
    "                    if url_req.status_code == 200:\n",
    "                        parsed = urlparse(url)\n",
    "                        domain = parsed.netloc\n",
    "                except:\n",
    "                    url = None\n",
    "\n",
    "                #If the url didn't get the domain, we can assign it if Hackernews described it.\n",
    "                if domain == None:\n",
    "                    domain = domain_new\n",
    "\n",
    "                #for multiple comments\n",
    "                mat = re.search(r'\\bcomments\\b', comment_soup.text.lower())\n",
    "                #for 1 comment\n",
    "                mat1 = re.search(r'\\bcomment\\b', comment_soup.text.lower())\n",
    "                if mat:   \n",
    "                    #Get the amount of comments by mirroring and starting to count backwards (by mirroring it), \n",
    "                    #in which case it is the first number.\n",
    "                    comment_number = re.search(r'\\d+', comment_soup.text[::-1])\n",
    "                    number = int(comment_number.group())\n",
    "                    #Mirror the number it back\n",
    "                    comment = str(number)[::-1]\n",
    "                #There were some slight complications with comment(s) so I decided to make 2 of them.\n",
    "                #1 with and 1 without s.\n",
    "                elif mat1: \n",
    "                    comment = '1'\n",
    "                else:\n",
    "                    comment = '0' \n",
    "\n",
    "\n",
    "                #For Scraping\n",
    "                try: \n",
    "                    Website_url = requests.get(url)\n",
    "                    soup = BeautifulSoup(Website_url.text, 'html.parser') \n",
    "                    #Source title scraping\n",
    "                    source_title_new = soup.find('h1', class_=contains_desired_class)\n",
    "                    if source_title_new == None: \n",
    "                        source_title_new = soup.find('h1')\n",
    "                        if source_title_new == None:\n",
    "                            source_title_new = soup.find('h3')\n",
    "                    source_title = source_title_new.get_text(strip=True) \n",
    "                    #Source text scraping \n",
    "                    source_text_temp = soup.find_all('p')\n",
    "                    source_text = source_text_temp[0].get_text(strip=True)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data = {\n",
    "                    \"Title\" : title, \n",
    "                    \"URL\" : url,\n",
    "                    \"Domain\" : domain,\n",
    "                    \"Posted_at\": posted_at_date,\n",
    "                    \"User\" : user, \n",
    "                    \"Votes\" : votes,\n",
    "                    \"Comments\" : comment,\n",
    "                    \"Source_title\" : source_title,\n",
    "                    \"Source_text\" : source_text\n",
    "                }\n",
    "\n",
    "                #add the file, it went through because the posted_at was right and title/user were not dupes.\n",
    "                all_data.append(data) \n",
    "                print(\"Appended1: \" + title + \" & \" + str(posted_at_date)) #interesting to see if the entry was recorded and when\n",
    "        #If there is a problem with the table/website, too many requests probably\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "            break\n",
    "\n",
    "        more = table.find('a', attrs={'class':'morelink'}) \n",
    "        hrefmore = more.get('href')\n",
    "        print(hrefmore)\n",
    "\n",
    "        Website = requests.get(\"https://news.ycombinator.com/\" + hrefmore)\n",
    "        soup = BeautifulSoup(Website.text, 'html.parser') \n",
    "\n",
    "    #sort on age from youngest to oldest, because then we will find the duplicates the fastest\n",
    "    sorted_data = sorted(all_data, key=lambda directory: directory['Posted_at'])\n",
    "    with open(f\"New_file{g+1}.json\", \"w\") as save_file:  \n",
    "            json.dump(sorted_data, save_file, indent = 9)\n",
    "\n",
    "\n",
    "    # Now we go to the FrontPage\n",
    "    print(\"---------------\")\n",
    "\n",
    "    # Appoint the website (Frontpage)\n",
    "    Website1 = requests.get(\"https://news.ycombinator.com/news\")\n",
    "    soup = BeautifulSoup(Website1.text, 'html.parser')    \n",
    "\n",
    "    #Make an all_data_front list\n",
    "    all_data_front = []\n",
    "    #Take the saved data in a list to all_data_front\n",
    "    with open(f\"Front_File{g}.json\", \"r\") as save_file:  \n",
    "        all_data_front = json.load(save_file)    \n",
    "\n",
    "    #FRONTPAGE\n",
    "    #After 5 pages, there will be nothing not 1d+ old, I took 8 pages to be sure.\n",
    "    for i in range(8):\n",
    "        time.sleep(10)\n",
    "        table = soup.find('table')\n",
    "        if table:\n",
    "            for index, (title_soup,posted_at_new,user_new,votes_new,comment_soup) in enumerate(zip(table.find_all('span', attrs={'class':'titleline'}),\n",
    "                table.find_all('span', attrs={'class':'age'}), table.find_all('a', attrs={'class':'hnuser'}),\n",
    "                table.find_all('span', attrs={'class':'score'}), table.find_all('td', attrs={'class':'subtext'}))):\n",
    "                #Put all values back to Null\n",
    "                title = None\n",
    "                url = None\n",
    "                domain = None\n",
    "                posted_at = None\n",
    "                user = None\n",
    "                votes = None \n",
    "                comment = None\n",
    "                source_title = None\n",
    "                source_text = None\n",
    "                continue_cond = False\n",
    "\n",
    "                #Define posted_at first, so we can skip the whole loop if not relevant\n",
    "                posted_at = age_to_time(posted_at_new.get_text())\n",
    "                if posted_at < 721:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"Skipped\") #Interesting to see whether the entry was skipped or not\n",
    "                    continue #Skip this particular post bc it is not relevant\n",
    "                    \n",
    "                #The date the post was gathered\n",
    "                post_date = current_date - timedelta(minutes = posted_at)\n",
    "                posted_at_date = post_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                #Find the Title\n",
    "                match = re.match(r\"^(.*?) (?:\\[(?:video|.+?)\\] )?\\(([a-zA-Z].*?)\\)\", title_soup.text)\n",
    "                if match:\n",
    "                    title, domain_new = match.groups()\n",
    "                else:\n",
    "                    title = title_soup.text\n",
    "                    domain_new = None\n",
    "\n",
    "                #Define the User and Vote\n",
    "                user = user_new.get_text()\n",
    "                votes = votes_new.get_text()\n",
    "\n",
    "                #go through the file, and if it is a dupe stop the whole while loop.\n",
    "                for item in all_data_front:\n",
    "                    if item['Title'] == title and item['User'] == user:\n",
    "                        #print(\"Dupe\")\n",
    "                        continue_cond = True\n",
    "                        break\n",
    "\n",
    "                #I do it this way, otherwise it would only break the nested loop, not the table one.\n",
    "                #If it is an index 0, 1 or 2 (the first items), there is still a possibility\n",
    "                #too much time passed scraping and the post went to the next page\n",
    "                #but the other posts on the page are relevant\n",
    "                if continue_cond == True:\n",
    "                    continue\n",
    "\n",
    "                #Find the Url\n",
    "                title_link = title_soup.find('a')\n",
    "                if title_link:\n",
    "                    url = title_link['href']\n",
    "                try: \n",
    "                    url_req = requests.get(url)\n",
    "                    if url_req.status_code == 200:\n",
    "                        parsed = urlparse(url)\n",
    "                        domain = parsed.netloc\n",
    "                except:\n",
    "                    url = None\n",
    "\n",
    "                #If the url didn't get the domain, we can assign it if Hackernews described it.\n",
    "                if domain == None:\n",
    "                    domain = domain_new\n",
    "\n",
    "                #for multiple comments\n",
    "                mat = re.search(r'\\bcomments\\b', comment_soup.text.lower())\n",
    "                #for 1 comment\n",
    "                mat1 = re.search(r'\\bcomment\\b', comment_soup.text.lower())\n",
    "                if mat:   \n",
    "                    #Get the amount of comments by mirroring and starting to count backwards (by mirroring it), \n",
    "                    #in which case it is the first number.\n",
    "                    comment_number = re.search(r'\\d+', comment_soup.text[::-1])\n",
    "                    number = int(comment_number.group())\n",
    "                    #Mirror the number it back\n",
    "                    comment = str(number)[::-1]\n",
    "                #There were some slight complications with comment(s) so I decided to make 2 of them.\n",
    "                #1 with and 1 without s.\n",
    "                elif mat1: \n",
    "                    comment = '1'\n",
    "                else:\n",
    "                    comment = '0' \n",
    "\n",
    "\n",
    "                #For Scraping\n",
    "                try: \n",
    "                    Website_url = requests.get(url)\n",
    "                    soup = BeautifulSoup(Website_url.text, 'html.parser') \n",
    "                    #Source title scraping\n",
    "                    source_title_new = soup.find('h1', class_=contains_desired_class)\n",
    "                    if source_title_new == None: \n",
    "                        source_title_new = soup.find('h1')\n",
    "                        if source_title_new == None:\n",
    "                            source_title_new = soup.find('h3')\n",
    "                    #Source text scraping \n",
    "                    source_text_temp = soup.find_all('p')\n",
    "                    source_text = source_text_temp[0].get_text(strip=True)\n",
    "                    #for source_text in soup.findAll('p'):\n",
    "                        #source_text_temp = []\n",
    "                        #append to temp \n",
    "                        #print(source_text)\n",
    "                        #source_text_temp.append(source_text.get_text(strip = True))\n",
    "                    #source_text = ' '.join(source_text_temp)\n",
    "                    #Now we add them to the lists  \n",
    "                    source_title = source_title_new.get_text(strip=True)       \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data = {\n",
    "                   \"Title\" : title, \n",
    "                    \"URL\" : url,\n",
    "                    \"Domain\" : domain,\n",
    "                    \"Posted_at\": posted_at_date,\n",
    "                    \"User\" : user, \n",
    "                    \"Votes\" : votes,\n",
    "                    \"Comments\" : comment,\n",
    "                    \"Source_title\" : source_title,\n",
    "                    \"Source_text\" : source_text\n",
    "                }\n",
    "\n",
    "                #add the file, it went through because the posted_at was right and title/user were not dupes.\n",
    "                all_data_front.append(data) \n",
    "                print(\"Appended2: \" + title + \" & \" + str(posted_at_date)) #interesting to see if the entry was recorded and when\n",
    "        #If there is a problem with the table/website, too many requests probably\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "            break\n",
    "        #If there is any problem getting to the next website, we just get the same data, \n",
    "        #Meaning we will get a cond1 False\n",
    "        more = table.find('a', attrs={'class':'morelink'}) \n",
    "        hrefmore = more.get('href')\n",
    "        print(hrefmore)\n",
    "\n",
    "        Website1 = requests.get(\"https://news.ycombinator.com/\" + hrefmore)\n",
    "        soup = BeautifulSoup(Website1.text, 'html.parser') \n",
    "\n",
    "    #Just save the data, sorting on age will do nothing here\n",
    "    with open(f\"Front_File{g+1}.json\", \"w\") as save_file:  \n",
    "        json.dump(all_data_front, save_file, indent = 9)    \n",
    "    print(\"Ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when, row_number, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+--------------+-----+--------+--------------------+--------------------+---------+\n",
      "|               title|                 url|             domain|          posted_at|          user|votes|comments|        source_title|         source_text|frontpage|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------+-----+--------+--------------------+--------------------+---------+\n",
      "|Developers' Perce...|https://arxiv.org...|          arxiv.org|2024-05-21 14:22:39|        belter|    2|       0|Title:Developers'...|Help|Advanced Search|        0|\n",
      "|Scale AI raises $...|https://techcrunc...|     techcrunch.com|2024-05-21 14:22:39|danielmorozoff|    3|       0|Data-labeling sta...|             Comment|        0|\n",
      "|Increasing LLM Ac...|https://arxiv.org...|          arxiv.org|2024-05-21 14:22:39|        belter|    1|       0|Title:Increasing ...|Help|Advanced Search|        0|\n",
      "|Common Lisp for S...|https://simonsafa...|     simonsafar.com|2024-05-21 14:22:39|  todsacerdoti|    3|       0|Common Lisp for s...|... let's use SBC...|        0|\n",
      "|Microplastics fou...|https://www.thegu...|www.theguardian.com|2024-05-21 14:22:39|      ceejayoz|   22|       3|Microplastics fou...|Scientists say di...|        0|\n",
      "|Lamucal.ai: Vocal...| https://lamucal.ai/|         lamucal.ai|2024-05-21 14:22:39|         CMLab|    3|       0|Any Song, Now AI ...|                NULL|        0|\n",
      "|Functional TypeSc...|https://injuly.in...|          injuly.in|2024-05-21 14:22:39|  todsacerdoti|    1|       0|Functional TypeSc...|I write Go and Ty...|        0|\n",
      "|   Scriptio Continua|https://en.wikipe...|   en.wikipedia.org|2024-05-21 14:22:39|   thunderbong|    1|       0|   Scriptio continua|Scriptio continua...|        0|\n",
      "|PM apologises aft...|https://www.bbc.c...|      www.bbc.co.uk|2024-05-21 14:22:39|      zeristor|    2|       1|PM apologises aft...|This video can no...|        0|\n",
      "|Local/LAN Tibber ...|https://github.co...|         github.com|2024-05-21 14:22:39|        doener|    1|       0|    Provide feedback|We read every pie...|        0|\n",
      "|Run Your Own Mail...|https://www.kicks...|    kickstarter.com|2024-05-21 14:22:39|          dsr_|   11|       9|                 403|We apologize but ...|        0|\n",
      "|Brand New Model F...|https://www.model...|modelfkeyboards.com|2024-05-21 14:22:39|   r3trohack3r|    3|       1|                 403|Access to this re...|        0|\n",
      "|EU Council gives ...|https://techcrunc...|     techcrunch.com|2024-05-21 14:22:39| unripe_syntax|    1|       0|EU Council gives ...|             Comment|        0|\n",
      "|The Paradigm Shif...|https://www.rwx.c...|        www.rwx.com|2024-05-21 14:22:39|   thunderbong|    1|       0|The Paradigm Shif...|        byDan Manges|        0|\n",
      "|'Satoshi' imperso...|https://www.lawga...|   lawgazette.co.uk|2024-05-21 14:22:39|          geox|  104|      23|       403 Forbidden|                NULL|        0|\n",
      "|Show HN: I built ...|https://voicelark...|      voicelark.com|2024-05-21 14:22:39|        winiak|    2|       1|Real-time crypto ...|Our advanced AI-b...|        0|\n",
      "|We created the fi...|https://www.codiu...|      www.codium.ai|2024-05-21 14:22:39|       gronky_|  126|      37|We created the fi...|In February, Meta...|        0|\n",
      "|How I Made Google...|https://tedium.co...|          tedium.co|2024-05-21 14:22:39|        marban|    1|       0|Does One Line Fix...|Filed under:ai,da...|        0|\n",
      "|   Is Morality Real?|https://www.ahalb...|   www.ahalbert.com|2024-05-21 14:22:39|     ahalbert2|    2|       0|   Is Morality Real?|In PlatoâsRepub...|        0|\n",
      "|We Pay $6k for Pr...|https://macleans....|        macleans.ca|2024-05-21 14:22:39|      esfandia|    3|       0|Why We Pay $6,000...|“There’s no wonde...|        0|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------+-----+--------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[title: string, url: string, domain: string, posted_at: timestamp, user: string, votes: int, comments: bigint, source_title: string, source_text: string, frontpage: bigint]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a Spark Session\n",
    "spark = SparkSession.builder.appName(\"HackerNews\").getOrCreate()\n",
    "\n",
    "json_front = 'Front_file641.json'\n",
    "json_newest = 'New_file641.json'\n",
    "\n",
    "df_front = pd.read_json(json_front)\n",
    "df_new = pd.read_json(json_newest)\n",
    "\n",
    "#We make a label called frontpage to differentiate the posts\n",
    "df_front['frontpage'] = 1\n",
    "df_new['frontpage'] = 0\n",
    "\n",
    "#put the data together\n",
    "df = pd.concat([df_front, df_new])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Sort based on oldest as is required\n",
    "df = df.sort_values(by='Posted_at')\n",
    "\n",
    "#See that every value of 'Votes' has 'point' in the string\n",
    "df = df[df['Votes'].str.contains('point')]\n",
    "\n",
    "#df.dropna(inplace = True)\n",
    "columns = df.columns.difference(['Source_text'])\n",
    "df.dropna(subset=columns, inplace= True, how='any')\n",
    "\n",
    "psdf = spark.createDataFrame(df)\n",
    "\n",
    "#We make the votes into integers\n",
    "def points_to_int(i):\n",
    "    if 'point' in i:\n",
    "        return int(i.split()[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Make a UDF\n",
    "points_to_int_udf = udf(points_to_int, IntegerType())\n",
    "\n",
    "#Put them in the Votes column\n",
    "psdf = psdf.withColumn('Votes', points_to_int_udf(col('Votes')))\n",
    "\n",
    "#put the titles in lower case, to match the model later on.\n",
    "def rename_columns_to_lowercase(df):\n",
    "    for title in df.columns:\n",
    "        df = df.withColumnRenamed(title, title[0].lower() + title[1:])\n",
    "    return df\n",
    "\n",
    "psdf = rename_columns_to_lowercase(psdf)\n",
    "psdf = psdf.withColumnRenamed('uRL', \"url\")\n",
    "\n",
    "psdf.show()\n",
    "psdf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [\"title\", \"url\", \"domain\", \"user\", \"source_title\"]\n",
    "numeric_columns = [\"votes\", \"comments\"]\n",
    "\n",
    "for col_name in string_columns:\n",
    "   psdf = psdf.withColumn(col_name, when(col(col_name) == \"\", None).otherwise(col(col_name)))\n",
    "psdf.na.drop()\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') for col in string_columns]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in string_columns]\n",
    "\n",
    "feature_columns = [col + \"_encoded\" for col in string_columns] + numeric_columns\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"frontpage\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train and test-set\n",
    "train_data, test_data = psdf.randomSplit([0.8,0.2], seed = 54321)\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "model.save('LRmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7639509255105427\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(\"LRmodel\")\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model (uncomment below):\n",
    "    \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexf\\OneDrive\\Bureaublad\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-26 13:20:50 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|40478628|       0|thebignewsletter.com|    false|2024-05-25 23:30:53|Antitrust Enforce...|Antitrust Enforce...|Antitrust Enforce...|https://www.thebi...|toomuchtodo|    2|\n",
      "|40478648|       0|      independent.ie|    false|2024-05-25 23:35:46|The future of Big...|The future of Big...|The future of Big...|https://www.indep...|ewgfdgdfgdf|    1|\n",
      "|40478665|       0|         youtube.com|    false|2024-05-25 23:39:47|How are Microchip...|How are Microchip...|How are microchip...|https://www.youtu...|webwielder2|    2|\n",
      "|40478667|       0|         cuelang.org|    false|2024-05-25 23:40:14|The Logic of CUE ...|    The Logic of CUE|    The Logic of CUE|https://cuelang.o...|  teleforce|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|     user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478628|       0|thebignewsletter.com|    false|2024-05-25 23:30:53|Antitrust Enforce...|Antitrust Enforce...|Antitrust Enforce...|https://www.thebi...|toomuchtodo|    2|     1974.0|   1900.0|      1107.0|      28.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[28],[1.0])|        (1583,[],[])|(7693,[5009,7691]...|[42.7944050218348...|           [1.0,0.0]|       0.0|\n",
      "|40478648|       0|      independent.ie|    false|2024-05-25 23:35:46|The future of Big...|The future of Big...|The future of Big...|https://www.indep...|ewgfdgdfgdf|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478665|       0|         youtube.com|    false|2024-05-25 23:39:47|How are Microchip...|How are Microchip...|How are microchip...|https://www.youtu...|webwielder2|    2|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[2.0])|[33.3315521697569...|[0.99999999999999...|       0.0|\n",
      "|40478667|       0|         cuelang.org|    false|2024-05-25 23:40:14|The Logic of CUE ...|    The Logic of CUE|    The Logic of CUE|https://cuelang.o...|  teleforce|    1|     1974.0|   1900.0|      1107.0|      52.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[52],[1.0])|        (1583,[],[])|(7693,[5033,7691]...|[24.0266763721358...|[0.99999999996324...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:00 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|40478669|       0|            phlux.us|    false|2024-05-25 23:40:29|Juice Sucking Ser...|Juice Sucking Ser...|Juice Sucking Ser...|https://tech.phlu...|  MBCook|    1|\n",
      "|40478696|       0|surfingcomplexity...|    false|2024-05-25 23:45:39|The error term is...|The error term is...|The error term is...|https://surfingco...|azhenley|    1|\n",
      "|40478702|       0|     simonsarris.com|    false|2024-05-25 23:47:29|Careful technolog...|  Careful technology|  Careful Technology|https://map.simon...|  voisin|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|     user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478669|       0|            phlux.us|    false|2024-05-25 23:40:29|Juice Sucking Ser...|Juice Sucking Ser...|Juice Sucking Ser...|https://tech.phlu...|  MBCook|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478696|       0|surfingcomplexity...|    false|2024-05-25 23:45:39|The error term is...|The error term is...|The error term is...|https://surfingco...|azhenley|    1|     1974.0|   1900.0|      1107.0|      41.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[41],[1.0])|        (1583,[],[])|(7693,[5022,7691]...|[33.8192914554791...|[0.99999999999999...|       0.0|\n",
      "|40478702|       0|     simonsarris.com|    false|2024-05-25 23:47:29|Careful technolog...|  Careful technology|  Careful Technology|https://map.simon...|  voisin|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:10 =========\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|40478707|       0|   fruitful.com|    false|2024-05-25 23:49:37|Fruitful - Financ...|Fruitful - Financ...|            Fruitful|https://www.fruit...|handfuloflight|    1|\n",
      "|40478712|       0|effectgames.com|    false|2024-05-25 23:51:46|Canvas Cycle: Tru...|Canvas Cycle: Tru...|Living Worlds: Ol...|http://www.effect...|   smusamashah|    1|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|      user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478707|       0|   fruitful.com|    false|2024-05-25 23:49:37|Fruitful - Financ...|Fruitful - Financ...|            Fruitful|https://www.fruit...|handfuloflight|    1|     1974.0|   1900.0|      1107.0|      24.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])| (1127,[24],[1.0])|        (1583,[],[])|(7693,[5005,7691]...|[41.2109972356128...|           [1.0,0.0]|       0.0|\n",
      "|40478712|       0|effectgames.com|    false|2024-05-25 23:51:46|Canvas Cycle: Tru...|Canvas Cycle: Tru...|Living Worlds: Ol...|http://www.effect...|   smusamashah|    1|     1974.0|   1900.0|      1107.0|     159.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[159],[1.0])|        (1583,[],[])|(7693,[5140,7691]...|[-49.342208261312...|[3.72349887694915...|       1.0|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:20 =========\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|40478721|       0|github.com/n64recomp|    false|2024-05-25 23:53:13|GitHub - N64Recom...|GitHub - N64Recom...|     N64: Recompiled|https://github.co...|      linksbro|    1|\n",
      "|40478733|       0|        casetext.com|    false|2024-05-25 23:55:15|Just a moment...\\...|    Just a moment...|Casetext: AI Lega...|https://casetext....|handfuloflight|    2|\n",
      "|40478735|       0|github.com/rochus...|    false|2024-05-25 23:55:37|GitHub - rochus-k...|GitHub - rochus-k...|CrossLine – outli...|https://github.co...|   karencarits|    1|\n",
      "|40478745|       0|         youtube.com|    false|2024-05-25 23:56:55|I Made a Neural N...|I Made a Neural N...|A Neural Network ...|https://www.youtu...|       yamrzou|    1|\n",
      "|40478781|       0|         openjdk.org|    false|2024-05-26 00:05:44|openjdk.org\\n\\n# ...|         openjdk.org|JEP 471: Deprecat...|https://openjdk.o...|         alasr|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|     user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478721|       0|github.com/n64recomp|    false|2024-05-25 23:53:13|GitHub - N64Recom...|GitHub - N64Recom...|     N64: Recompiled|https://github.co...|      linksbro|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478733|       0|        casetext.com|    false|2024-05-25 23:55:15|Just a moment...\\...|    Just a moment...|Casetext: AI Lega...|https://casetext....|handfuloflight|    2|     1974.0|   1900.0|      1107.0|      24.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[24],[1.0])|        (1583,[],[])|(7693,[5005,7691]...|[41.4386872921509...|           [1.0,0.0]|       0.0|\n",
      "|40478735|       0|github.com/rochus...|    false|2024-05-25 23:55:37|GitHub - rochus-k...|GitHub - rochus-k...|CrossLine – outli...|https://github.co...|   karencarits|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478745|       0|         youtube.com|    false|2024-05-25 23:56:55|I Made a Neural N...|I Made a Neural N...|A Neural Network ...|https://www.youtu...|       yamrzou|    1|     1974.0|   1900.0|      1107.0|      74.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[74],[1.0])|        (1583,[],[])|(7693,[5055,7691]...|[33.9863981122873...|[0.99999999999999...|       0.0|\n",
      "|40478781|       0|         openjdk.org|    false|2024-05-26 00:05:44|openjdk.org\\n\\n# ...|         openjdk.org|JEP 471: Deprecat...|https://openjdk.o...|         alasr|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:30 =========\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+\n",
      "|     aid|comments|       domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|     user|votes|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+\n",
      "|40478809|       0|  philips.com|    false|2024-05-26 00:12:13|Capacitive microm...|Capacitive microm...|Capacitive Microm...|https://www.engin...|teleforce|    1|\n",
      "|40478819|       0|projectit.com|    false|2024-05-26 00:15:19|Free Rare Firefox...|Free Rare Firefox...|Historic Mozilla/...|https://projectit...|    Lammy|    1|\n",
      "|40478820|       0| qualcomm.com|    false|2024-05-26 00:15:20|AI-powered produc...|AI-powered produc...|AI-powered produc...|https://www.qualc...|   xgdgsc|    1|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+\n",
      "\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|       domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|     user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|      user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478809|       0|  philips.com|    false|2024-05-26 00:12:13|Capacitive microm...|Capacitive microm...|Capacitive Microm...|https://www.engin...|teleforce|    1|     1974.0|   1900.0|      1107.0|      52.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])| (1127,[52],[1.0])|        (1583,[],[])|(7693,[5033,7691]...|[24.0266763721358...|[0.99999999996324...|       0.0|\n",
      "|40478819|       0|projectit.com|    false|2024-05-26 00:15:19|Free Rare Firefox...|Free Rare Firefox...|Historic Mozilla/...|https://projectit...|    Lammy|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|      (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478820|       0| qualcomm.com|    false|2024-05-26 00:15:20|AI-powered produc...|AI-powered produc...|AI-powered produc...|https://www.qualc...|   xgdgsc|    1|     1974.0|   1900.0|      1107.0|     167.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[167],[1.0])|        (1583,[],[])|(7693,[5148,7691]...|[41.8264008059327...|           [1.0,0.0]|       0.0|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:40 =========\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|     aid|comments|    domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|40478835|       0|xerces.org|    false|2024-05-26 00:19:55|Just a moment...\\...|    Just a moment...|Pollinator-Friend...|https://xerces.or...|toomuchtodo|    1|\n",
      "|40478838|       0|   twz.com|     true|2024-05-26 00:20:41|China Stages Mock...|China Stages Mock...|China Stages Mock...|https://www.twz.c...|   peutetre|    6|\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+-----------+----------+\n",
      "|     aid|comments|    domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|     user_encoded|source_title_encoded|            features|       rawPrediction|probability|prediction|\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+-----------+----------+\n",
      "|40478835|       0|xerces.org|    false|2024-05-26 00:19:55|Just a moment...\\...|    Just a moment...|Pollinator-Friend...|https://xerces.or...|toomuchtodo|    1|     1974.0|   1900.0|      1107.0|      28.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[28],[1.0])|        (1583,[],[])|(7693,[5009,7691]...|[42.5667149652966...|  [1.0,0.0]|       0.0|\n",
      "|40478838|       0|   twz.com|     true|2024-05-26 00:20:41|China Stages Mock...|China Stages Mock...|China Stages Mock...|https://www.twz.c...|   peutetre|    6|     1974.0|   1900.0|      1107.0|      49.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[49],[1.0])|        (1583,[],[])|(7693,[5030,7691]...|[47.3498018109444...|  [1.0,0.0]|       0.0|\n",
      "+--------+--------+----------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+-----------+----------+\n",
      "\n",
      "========= 2024-05-26 13:21:50 =========\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|     aid|comments|      domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|40478853|       0|thedrive.com|    false|2024-05-26 00:25:49|Kenworth’s Wild S...|Kenworth’s Wild S...|Kenworth's SuperT...|https://www.thedr...|peutetre|    1|\n",
      "|40478867|       0| youtube.com|    false|2024-05-26 00:30:30|What's Going Wron...|What's Going Wron...|What's Going Wron...|https://www.youtu...| amelius|    1|\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|      domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|     user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478853|       0|thedrive.com|    false|2024-05-26 00:25:49|Kenworth’s Wild S...|Kenworth’s Wild S...|Kenworth's SuperT...|https://www.thedr...|peutetre|    1|     1974.0|   1900.0|      1107.0|      49.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[49],[1.0])|        (1583,[],[])|(7693,[5030,7691]...|[46.2113515282535...|           [1.0,0.0]|       0.0|\n",
      "|40478867|       0| youtube.com|    false|2024-05-26 00:30:30|What's Going Wron...|What's Going Wron...|What's Going Wron...|https://www.youtu...| amelius|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|     (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "+--------+--------+------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:22:00 =========\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|     aid|comments|          domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|40478871|       0|skilldeliver.com|    false|2024-05-26 00:33:40|One Page E-Shop\\n...|     One Page E-Shop|     One Page E-Shop|https://blog.skil...|skilldeliver|    1|\n",
      "|40478881|       0|  wickstrom.tech|    false|2024-05-26 00:36:19|Statically Typed ...|Statically Typed ...|Statically Typed ...|https://wickstrom...|      JNRowe|    1|\n",
      "|40478885|       0|     proboat.com|    false|2024-05-26 00:36:47|Electric Foiling ...|Electric Foiling ...|Electric Foiling ...|https://www.probo...|   troydavis|    1|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-----------+---------+------------+----------+------------------+-------------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|          domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|title_index|url_index|domain_index|user_index|source_title_index|      title_encoded|        url_encoded|    domain_encoded|user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-----------+---------+------------+----------+------------------+-------------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478871|       0|skilldeliver.com|    false|2024-05-26 00:33:40|One Page E-Shop\\n...|     One Page E-Shop|     One Page E-Shop|https://blog.skil...|skilldeliver|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0|       (1974,[],[])|       (1900,[],[])|      (1107,[],[])|(1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478881|       0|  wickstrom.tech|    false|2024-05-26 00:36:19|Statically Typed ...|Statically Typed ...|Statically Typed ...|https://wickstrom...|      JNRowe|    1|     1599.0|   1275.0|       830.0|    1127.0|            1583.0|(1974,[1599],[1.0])|(1900,[1275],[1.0])|(1107,[830],[1.0])|(1127,[],[])|        (1583,[],[])|(7693,[1599,3249,...|[65.6761939020728...|           [1.0,0.0]|       0.0|\n",
      "|40478885|       0|     proboat.com|    false|2024-05-26 00:36:47|Electric Foiling ...|Electric Foiling ...|Electric Foiling ...|https://www.probo...|   troydavis|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0|       (1974,[],[])|       (1900,[],[])|      (1107,[],[])|(1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-----------+---------+------------+----------+------------------+-------------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:22:10 =========\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|     aid|comments|          domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|40478898|       0|         cnn.com|     true|2024-05-26 00:39:44|edition.cnn.com\\n...|     edition.cnn.com|Record number of ...|https://www.cnn.c...|      caprock|    5|\n",
      "|40478905|       0|      artima.com|    false|2024-05-26 00:41:04|Origin of BDFL\\n\\...|      Origin of BDFL|Origin of BDFL (2...|https://www.artim...|       ibobev|    1|\n",
      "|40478911|       0|eleanorkonik.com|    false|2024-05-26 00:42:02|🙈 Downsides of s...|🙈 Downsides of s...|Downsides of surr...|https://www.elean...|maxwelljoslyn|    1|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|          domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|      user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478898|       0|         cnn.com|     true|2024-05-26 00:39:44|edition.cnn.com\\n...|     edition.cnn.com|Record number of ...|https://www.cnn.c...|      caprock|    5|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|      (1127,[],[])|        (1583,[],[])| (7693,[7691],[5.0])|[34.0146223393714...|[0.99999999999999...|       0.0|\n",
      "|40478905|       0|      artima.com|    false|2024-05-26 00:41:04|Origin of BDFL\\n\\...|      Origin of BDFL|Origin of BDFL (2...|https://www.artim...|       ibobev|    1|     1974.0|   1900.0|      1107.0|     732.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[732],[1.0])|        (1583,[],[])|(7693,[5713,7691]...|[40.7812477838306...|           [1.0,0.0]|       0.0|\n",
      "|40478911|       0|eleanorkonik.com|    false|2024-05-26 00:42:02|🙈 Downsides of s...|🙈 Downsides of s...|Downsides of surr...|https://www.elean...|maxwelljoslyn|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|      (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 13:22:20 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|40478923|       0|             bbc.com|     true|2024-05-26 00:45:04|'I was misidentif...|'I was misidentif...|'I was misidentif...|https://www.bbc.c...| gnabgib|   10|\n",
      "|40478937|       0|  textslashplain.com|    false|2024-05-26 00:48:00|Authenticode in 2...|Authenticode in 2024|Authenticode in 2024|https://textslash...|aa_is_op|    1|\n",
      "|40478948|       0|befinitiv.wordpre...|    false|2024-05-26 00:50:16|8cm Portable Blur...|8cm Portable Blur...|8cm Portable Blur...|https://befinitiv...|     zdw|    2|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|title_index|url_index|domain_index|user_index|source_title_index|title_encoded| url_encoded|domain_encoded|      user_encoded|source_title_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478923|       0|             bbc.com|     true|2024-05-26 00:45:04|'I was misidentif...|'I was misidentif...|'I was misidentif...|https://www.bbc.c...| gnabgib|   10|     1974.0|   1900.0|      1107.0|      11.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])| (1127,[11],[1.0])|        (1583,[],[])|(7693,[4992,7691]...|[-9.7232665293570...|[5.98705155644669...|       1.0|\n",
      "|40478937|       0|  textslashplain.com|    false|2024-05-26 00:48:00|Authenticode in 2...|Authenticode in 2024|Authenticode in 2024|https://textslash...|aa_is_op|    1|     1974.0|   1900.0|      1107.0|    1127.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|      (1127,[],[])|        (1583,[],[])| (7693,[7691],[1.0])|[33.1038621132187...|[0.99999999999999...|       0.0|\n",
      "|40478948|       0|befinitiv.wordpre...|    false|2024-05-26 00:50:16|8cm Portable Blur...|8cm Portable Blur...|8cm Portable Blur...|https://befinitiv...|     zdw|    2|     1974.0|   1900.0|      1107.0|     168.0|            1583.0| (1974,[],[])|(1900,[],[])|  (1107,[],[])|(1127,[168],[1.0])|        (1583,[],[])|(7693,[5149,7691]...|[44.9015975174515...|           [1.0,0.0]|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----------+---------+------------+----------+------------------+-------------+------------+--------------+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
